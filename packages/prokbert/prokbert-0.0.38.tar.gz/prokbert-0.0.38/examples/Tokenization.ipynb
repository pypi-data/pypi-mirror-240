{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d169ce5-e121-48c4-a97d-06d6ce1a03b8",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "This notebooks will present the tokenization of sequence for prokbert.\n",
    "Parts are:\n",
    "  * Tokenization process background and paramters\n",
    "  * Tokenization of sequences\n",
    "  * Tokenization for pretraining\n",
    "  * HDF datasets for storing preprocessed sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d384f0-2b14-48da-a9a9-ed7d76b427c0",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenization of Sequence Data\n",
    "\n",
    "ProkBERT employs LCA tokenization, leveraging overlapping k-mers to capture rich local context information, enhancing model generalization and performance. The key parameters are the k-mer size and shift. For instance, with a k-mer size of 6 and a shift of 1, the tokenization captures detailed sequence information, while a k-mer size of 1 represents a basic character-based approach.\n",
    "\n",
    "### Segmentation Strategies\n",
    "Before tokenization, sequences are segmented using two main approaches:\n",
    "1. **Contiguous Sampling**: Divides contigs into non-overlapping segments.\n",
    "2. **Random Sampling**: Fragments the input sequence into randomly sized segments.\n",
    "\n",
    "### Tokenization Process\n",
    "After segmentation, sequences are encoded into a simpler vector format. The LCA method is pivotal in this phase, allowing the model to use a broader context and reducing computational demands while maintaining the information-rich local context.\n",
    "\n",
    "### Context Size Limitations\n",
    "It's important to note that transformer models, including ProkBERT, have a context size limitation. ProkBERT's design accommodates context sizes significantly larger than an average gene, yet smaller than the average bacterial genome.\n",
    "\n",
    "We provide pretrained models for variants like ProkBERT-mini (k-mer size 6, shift 1), ProkBERT-mini-c (k-mer size 1, shift 1), and ProkBERT-mini-long (k-mer size 6, shift 2), catering to different sequence analysis requirements.\n",
    "\n",
    "<img src=\"https://github.com/nbrg-ppcu/prokbert/blob/main/assets/Figure2_tokenization.png?raw=true\" width=\"800\" alt=\"Segmentation Process\"> \n",
    "\n",
    "*Figure: The tokenization process in ProkBERT.*\n",
    "\n",
    "It is important to see that, when the $shift > 1$ there are multiple possible tokenization, depending where we start the tokenizaiton. The window offset refers the actual tokenization window. \n",
    "I.e. let the sequence be `ATGTCCGCGACCTTTCATACATACCACCGGTAC` with the k-mer size 6, shift 2) we will have two possible tokenization:\n",
    "\n",
    "Tokenization with offset=0:\n",
    "```plaintext\n",
    "    ATGTCCGCGACCTTTCATACATACCACCGGTAC\n",
    "0.  ATGTCC  GACCTT  ATACAT  CACCGG\n",
    "1.    GTCCGC  CCTTTC  ACATAC  CCGGTA\n",
    "2.      CCGCGA  TTTCAT  ATACCA\n",
    "3.        GCGACC  TCATAC  ACCACC\n",
    "```\n",
    "Tokenization with offset=1\n",
    "```plaintext\n",
    "    ATGTCCGCGACCTTTCATACATACCACCGGTAC\n",
    "0.   TGTCCG  ACCTTT  TACATA  ACCGGT\n",
    "1.     TCCGCG  CTTTCA  CATACC  CGGTAC\n",
    "2.       CGCGAC  TTCATA  TACCAC\n",
    "3.         CGACCT  CATACA  CCACCG\n",
    "```\n",
    "By default all possible tokenization is returned. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c345ea8-d8a0-4d57-90c9-7d36bc9dabd9",
   "metadata": {},
   "source": [
    "## Tokenization parameters\n",
    "\n",
    "The following table outlines the configuration parameters for ProkBERT, detailing their purpose, default values, types, and constraints.\n",
    "\n",
    "\n",
    "| Parameter | Description | Type | Default | Constraints |\n",
    "|-----------|-------------|------|---------|-------------|\n",
    "| **Tokenization** |\n",
    "| `type` | Describes the tokenization approach. By default, the LCA (Local Context Aware) method is used. | string | `lca` | Options: `lca` |\n",
    "| `kmer` | Determines the k-mer size for the tokenization process. | integer | 6 | Options: 1-9 |\n",
    "| `shift` | Represents the shift parameter in k-mer. The default value is 1. | integer | 1 | Min: 0 |\n",
    "| `max_segment_length` | Gives the maximum number of characters in a segment. This should be consistent with the language model's capability. It can be alternated with token_limit. | integer | 2050 | Min: 6, Max: 4294967296 |\n",
    "| `token_limit` | States the maximum token count that the language model can process, inclusive of special tokens like CLS and SEP. This is interchangeable with max_segment_length. | integer | 4096 | Min: 1, Max: 4294967296 |\n",
    "| `max_unknown_token_proportion` | Defines the maximum allowed proportion of unknown tokens in a sequence. For instance, if 10% of the tokens are unknown (when max_unknown_token_proportion=0.1), the segment won't be tokenized. | float | 0.9999 | Min: 0, Max: 1 |\n",
    "| `vocabfile` | Path to the vocabulary file. If set to 'auto', the default vocabulary is utilized. | str | `auto` | - |\n",
    "| `vocabmap` | The default vocabmap loaded from file | dict | `{}` | - |\n",
    "| `isPaddingToMaxLength` | Determines if the tokenized sentence should be padded with [PAD] tokens to produce vectors of a fixed length. | bool | False | Options: True, False |\n",
    "| `add_special_token` | The tokenizer should add the special starting and sentence end tokens. The default is yes. | bool | True | Options: True, False |\n",
    "| **Computation** |\n",
    "| `cpu_cores_for_segmentation` | Specifies the number of CPU cores allocated for the segmentation process. | integer | 10 | Min: 1 |\n",
    "| `cpu_cores_for_tokenization` | Allocates a certain number of CPU cores for the k-mer tokenization process. | integer | -1 | Min: 1 |\n",
    "| `batch_size_tokenization` | Determines the number of segments a single core processes at a time. The input segment list will be divided into chunks of this size. | integer | 10000 | Min: 1 |\n",
    "| `batch_size_fasta_segmentation` | Sets the number of fasta files processed in a single batch, useful when dealing with a large number of fasta files. | integer | 3 | Min: 1 |\n",
    "| `numpy_token_integer_prec_byte` | The type of integer to be used during the vectorization. The default is 2, if you want to work larger k-mers then increase it to 4. 1: np.int8, 2:np.int16. 4:np.int32. 8: np.int64 | integer | 2 | Options: 1, 2, 4, 8 |\n",
    "| `np_tokentype` | Dummy | type | `np.int16` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576a136-a8e7-4d4c-af14-2264b8995e6f",
   "metadata": {},
   "source": [
    "## Installation of ProkBERT (if needed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66218845-48e1-4e3e-af3c-b4e31411e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProkBERT is already installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import prokbert\n",
    "    print(\"ProkBERT is already installed.\")\n",
    "except ImportError:\n",
    "    !pip install prokbert\n",
    "    print(\"Installed ProkBERT.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488de6fa-248a-47fa-be82-f4597c62951e",
   "metadata": {},
   "source": [
    "# Tokenization of sequences examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e72e9bf-f782-4cc2-8fc3-efefab5e2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 11:21:22,443 - INFO - Nr. line to cover the seq:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 954, 2910, 1437, 2442, 3] [2, 3803, 3435, 1638, 1564, 3]\n",
      "ATGTCC GTCCGC CCGCGA GCGACC\n",
      "    ATGTCCGCGACCT\n",
      "0.  ATGTCC\n",
      "1.    GTCCGC\n",
      "2.      CCGCGA\n",
      "3.        GCGACC\n"
     ]
    }
   ],
   "source": [
    "from prokbert.config_utils import *\n",
    "from prokbert.sequtils import *\n",
    "tokenization_parameters = {'kmer' : 6,\n",
    "                          'shift' : 2}\n",
    "\n",
    "segment = 'ATGTCCGCGACCT'\n",
    "defconfig = SeqConfig() # For the detailed configarion parameters see the table above\n",
    "tokenization_params = defconfig.get_and_set_tokenization_parameters(tokenization_parameters)\n",
    "tokens, kmers = lca_tokenize_segment(segment, tokenization_params)\n",
    "\n",
    "print(' '.join([str(t) for t in tokens]))\n",
    "print(' '.join(kmers[0]))\n",
    "\n",
    "results_pretty_print = pretty_print_overlapping_sequence(segment, kmers[0], tokenization_params)\n",
    "print(results_pretty_print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f35c62-1a37-474c-a7df-2508cccc7350",
   "metadata": {},
   "source": [
    "## Tokenizing longer sequences\n",
    "If you wish to tokenize longer sequences what the current prokBERT model supports, then adjust the `token_limit` and `max_segment_length` parameters.\n",
    "Note that tokenization process is paralleled using the python multiprocessing module at segment levels, you might to adjust the number of cores to be used as well as the `batch_size_tokenization` (how many sequence a core should process at once) otherwise you might run out of memory. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14fdb064-3e15-4422-bc86-fe8424a11c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_parameters = {'kmer' : 6,\n",
    "                          'shift' : 1,\n",
    "                          'max_segment_length': 2000000,\n",
    "                          'token_limit': 2000000}\n",
    "\n",
    "segment = 'ATGTCCGCGACCT'*100000 # long sequence\n",
    "defconfig = SeqConfig() # For the detailed configarion parameters see the table above\n",
    "tokenization_params = defconfig.get_and_set_tokenization_parameters(tokenization_parameters)\n",
    "tokens, kmers = lca_tokenize_segment(segment, tokenization_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e07efc-f14e-4164-bf50-3ba036d2065d",
   "metadata": {},
   "source": [
    "### Tokenization with tokenizer class\n",
    "\n",
    "The tokenizer class can be used for tokenization as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f15ddb-c408-4531-9f31-3641f9b48414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c4990-95cc-4ef5-a7b9-3c443275b17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a9682-bdf1-4471-a732-8c1eb2ece17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1555776-593c-47ba-8463-b25e2dc7b371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f3a888-53db-4188-b4de-16e09752fce2",
   "metadata": {},
   "source": [
    "# Tokenization for pretraining\n",
    "\n",
    "The sequence data is largely abundant and it is not possible store in the memory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff931c-a184-449b-bd60-02ac2cce77ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a14cba-c7b2-470b-9f0f-4e544c588387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e600fb-7663-4ed6-b64f-a7fc9b257748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8ab92-9690-4c75-a23a-cae9adb02689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cabea51-7336-43fd-8aee-e52c4b557ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228a9fc-b2f6-4820-8176-b8f8b229449e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262cbc3-f3eb-450a-a7e7-9fc24e883f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
