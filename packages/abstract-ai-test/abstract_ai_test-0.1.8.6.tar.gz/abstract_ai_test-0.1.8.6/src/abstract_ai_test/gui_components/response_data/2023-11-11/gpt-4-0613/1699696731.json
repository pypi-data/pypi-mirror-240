{
    "prompt": {
        "model": "gpt-4",
        "messages": [
            {
                "role": "assistant",
                "content": "\n-----------------------------------------------------------------------------\n#instructions#\n\nyour response is expected to be in JSON format with the keys as follows:\n\n0) api_response - place response to prompt here\n1) notation - A useful parameter that allows a module to retain context and continuity of the prompts. These notations can be used to preserve relevant information or context that should be carried over to subsequent prompts.\n2) suggestions - A parameter that allows the module to provide suggestions for improving efficiency in future prompt sequences. These suggestions will be reviewed by the user after the entire prompt sequence is fulfilled.\n3) additional_responses - This parameter, usually set to True when the answer cannot be fully covered within the current token limit, initiates a loop that continues to send the current chunk's prompt until the module returns a False value. This option also enables a module to have access to previous notations\n4) abort - if you cannot fullfil the request, return this value True; be sure to leave a notation detailing whythis was\n5) generate_title - A parameter used for title generation of the chat. To maintain continuity, the generated title for a given sequence is shared with subsequent queries.\n6) request_chunks - you may request that the previous chunk data be prompted again, if selected, the query itterate once more with the previous chunk included in the prompt. return this value as True to impliment this option; leave sufficient notation as to why this was neccisary for the module recieving the next prompt\n7) prompt_as_previous - this is a user end declaration; if this is visible that means that the request portion of the prompt will change such that the previous response data will be available if needed\n8) token_adjustment - generally the token size of incoming queries is going to be static, because of this, if it will provide better results to increase or decrease the future allotment, place a number -100 up to 100 for a percentage adjustment for the remainder of the chunks and data\n\nbelow is an example of the expected json dictionary response format, with the default inputs:\n{\"api_response\": \"\", \"notation\": \"\", \"suggestions\": \"\", \"additional_responses\": False, \"abort\": False, \"generate_title\": \"\", \"request_chunks\": False, \"prompt_as_previous\": \"\", \"token_adjustment\": 0}\n-----------------------------------------------------------------------------\n#chunk_data#\n\nthis is chunk 1 of 1\n\n"
            }
        ],
        "max_tokens": 3896
    },
    "response": {
        "id": "chatcmpl-8Jf3LCQXoll03t6KZrwWiVBHdtN3F",
        "object": "chat.completion",
        "created": 1699696731,
        "model": "gpt-4-0613",
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "{\"api_response\": \"\", \"notation\": \"\", \"suggestions\": \"\", \"additional_responses\": False, \"abort\": False, \"generate_title\": \"\", \"request_chunks\": False, \"prompt_as_previous\": \"\", \"token_adjustment\": 0}"
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": 487,
            "completion_tokens": 50,
            "total_tokens": 537
        }
    },
    "file_path": "C:\\Users\\jrput\\Documents\\python projects\\Modules\\test_modules\\abstract_ai_test\\src\\abstract_ai_test\\gui_components\\response_data\\2023-11-11\\gpt-4-0613\\1699696731.json",
    "title": 1699696731,
    "model": "gpt-4-0613"
}